{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hello world\n",
    "\n",
    "In this unit you will learn how to use Python to implement the first ever program\n",
    "that *every* programmer starts with. This also serves as an example for the master\n",
    "notebook format.\n",
    "\n",
    "```\n",
    "# All markdown cells are searched for triple-backtick blocks. Within a triple quoted block,\n",
    "# a match on /^# ASSIGNMENT METADATA/ will trigger handling this as an assignment-level metadata\n",
    "# block. The rest of the metadata is parsed as YAML and may be used e.g. for setting\n",
    "# default settings for autograder isolation (memory limit etc).\n",
    "# The assignment_id is useful to identify which assignment a submission pertains to.\n",
    "# ASSIGNMENT METADATA\n",
    "assignment_id: \"HelloWorld\"\n",
    "```\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Here is the traditional first programming exercise, called \"Hello world\".\n",
    "The task is to print the message: \"Hello, world\".\n",
    "\n",
    "Here are a few examples to get you started. Run the following cells and see how\n",
    "you can print a message. To run a cell, click with mouse inside a cell, then\n",
    "press Ctrl+Enter to execute it. If you want to execute a few cells sequentially,\n",
    "then press Shift+Enter instead, and the focus will be automatically moved\n",
    "to the next cell as soon as one cell finishes execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bye bye\n"
     ]
    }
   ],
   "source": [
    "print(\"bye bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey you\n"
     ]
    }
   ],
   "source": [
    "print(\"hey\", \"you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "two\n"
     ]
    }
   ],
   "source": [
    "print(\"one\")\n",
    "print(\"two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MASTER ONLY. A few helper functions and IPython magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hidecode"
    ]
   },
   "outputs": [],
   "source": [
    "# MASTER ONLY\n",
    "import unittest\n",
    "\n",
    "# TODO(salikh): Move this into a shared library and make that library installable via pip.\n",
    "class SummaryTestResult(unittest.TextTestResult):\n",
    "    \"\"\"A small extension of TextTestResult that also collects a map of test statuses.\n",
    "    \n",
    "    result.results is a map from test name (string) to boolean: True(passed) or False(failed or error)\"\"\"\n",
    "    \n",
    "    separator1 = '=' * 70\n",
    "    separator2 = '-' * 70\n",
    "    \n",
    "    def __init__(self, stream, descriptions, verbosity):\n",
    "        super(unittest.TextTestResult, self).__init__(stream, descriptions, verbosity)\n",
    "        # A map of test name to True(passed) or False(failed or error)\n",
    "        self.results = {}\n",
    "        # Copied from TextTestResult.\n",
    "        self.stream = stream\n",
    "        self.showAll = verbosity > 1\n",
    "        self.dots = verbosity == 1\n",
    "        self.descriptions = descriptions\n",
    "\n",
    "    def testName(self, test):\n",
    "        \"\"\"A helper function to format the test as a human-readable string.\n",
    "        \n",
    "        The format is TestClassName.test_method. This is similar\n",
    "        to TextTestResult.getDescription(test), but uses different format.\n",
    "        getDescription: 'test_one (__main__.HelloTest)'\n",
    "        testName: 'HelloTest.test_one'\n",
    "        \"\"\"\n",
    "        return unittest.util.strclass(test.__class__).replace('__main__.', '') + '.' + test._testMethodName\n",
    "        \n",
    "    def getDescription(self, test):\n",
    "        doc_first_line = test.shortDescription()\n",
    "        if self.descriptions and doc_first_line:\n",
    "            return '\\n'.join((str(test), doc_first_line))\n",
    "        else:\n",
    "            return str(test)\n",
    "\n",
    "    def startTest(self, test):\n",
    "        super(unittest.TextTestResult, self).startTest(test)\n",
    "        if self.showAll:\n",
    "            self.stream.write(self.getDescription(test))\n",
    "            self.stream.write(\" ... \")\n",
    "            self.stream.flush()\n",
    "\n",
    "    def addSuccess(self, test):\n",
    "        super(unittest.TextTestResult, self).addSuccess(test)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"ok\")\n",
    "        elif self.dots:\n",
    "            self.stream.write('.')\n",
    "            self.stream.flush()\n",
    "        self.results[self.testName(test)] = True\n",
    "\n",
    "    def addError(self, test, err):\n",
    "        super(unittest.TextTestResult, self).addError(test, err)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"ERROR\")\n",
    "        elif self.dots:\n",
    "            self.stream.write('E')\n",
    "            self.stream.flush()\n",
    "        self.results[self.testName(test)] = False\n",
    "\n",
    "    def addFailure(self, test, err):\n",
    "        super(unittest.TextTestResult, self).addFailure(test, err)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"FAIL\")\n",
    "        elif self.dots:\n",
    "            self.stream.write('F')\n",
    "            self.stream.flush()\n",
    "        self.results[self.testName(test)] = False\n",
    "\n",
    "    def addSkip(self, test, reason):\n",
    "        super(unittest.TextTestResult, self).addSkip(test, reason)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"skipped {0!r}\".format(reason))\n",
    "        elif self.dots:\n",
    "            self.stream.write(\"s\")\n",
    "            self.stream.flush()\n",
    "\n",
    "    def addExpectedFailure(self, test, err):\n",
    "        super(unittest.TextTestResult, self).addExpectedFailure(test, err)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"expected failure\")\n",
    "        elif self.dots:\n",
    "            self.stream.write(\"x\")\n",
    "            self.stream.flush()\n",
    "\n",
    "    def addUnexpectedSuccess(self, test):\n",
    "        super(unittest.TextTestResult, self).addUnexpectedSuccess(test)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"unexpected success\")\n",
    "        elif self.dots:\n",
    "            self.stream.write(\"u\")\n",
    "            self.stream.flush()\n",
    "\n",
    "    def printErrors(self):\n",
    "        if self.dots or self.showAll:\n",
    "            self.stream.writeln()\n",
    "        self.printErrorList('ERROR', self.errors)\n",
    "        self.printErrorList('FAIL', self.failures)\n",
    "\n",
    "    def printErrorList(self, flavour, errors):\n",
    "        for test, err in errors:\n",
    "            self.stream.writeln(self.separator1)\n",
    "            self.stream.writeln(\"%s: %s\" % (flavour,self.getDescription(test)))\n",
    "            self.stream.writeln(self.separator2)\n",
    "            self.stream.writeln(\"%s\" % err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hidecode"
    ]
   },
   "outputs": [],
   "source": [
    "# MASTER ONLY. TODO(salikh): Extract the magics code into a pip library.\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "\n",
    "import re\n",
    "\n",
    "@register_cell_magic\n",
    "def submission(line, cell):\n",
    "    \"\"\"Registers a submission_source and submission, if the code can run.\n",
    "    \n",
    "    This magic is useful for auto-testing (testing autograder unit tests on incorrect inputs)\"\"\"\n",
    "    \n",
    "    # Copy the source into submission_source.source\n",
    "    globals()['submission_source'] = SimpleNamespace(source=cell)\n",
    "\n",
    "    env = {}\n",
    "    try:\n",
    "        exec(cell, globals(), env)\n",
    "    except Exception as e:\n",
    "        # Ignore execution errors -- just print them.\n",
    "        print('Exception while executing submission:\\n', e)\n",
    "        # If the code cannot be executed, leave the submission empty.\n",
    "        globals()['submission'] = None\n",
    "        return None\n",
    "    # Copy the modifications into submission object.\n",
    "    globals()['submission'] = SimpleNamespace(**env)\n",
    "\n",
    "@register_cell_magic\n",
    "def solution(line, cell):\n",
    "    \"\"\"Registers solution and evaluates it.\n",
    "    \n",
    "    Also removes the PROMPT block and %%solution from the solution source.\n",
    "    \n",
    "    The difference from %%submission is that the solution is copied into the top context,\n",
    "    making it possible to refer to the functions and variables in subsequent notebook cells.\"\"\"\n",
    "    \n",
    "    # Cut out PROMPT\n",
    "    cell = re.sub('(?ms)[ \\t]*\"\"\" # BEGIN PROMPT.*\"\"\" # END PROMPT[ \\t]*\\n?', '', cell)\n",
    "    # Cut out BEGIN/END SOLUTION markers\n",
    "    cell = re.sub('(?ms)[ \\t]*# (BEGIN|END) SOLUTION[ \\t]*\\n?', '', cell)\n",
    "\n",
    "    # Copy the source into submission_source.source\n",
    "    globals()['submission_source'] = SimpleNamespace(source=cell)\n",
    "    \n",
    "    env = {}\n",
    "    # Note: if solution throws exception, this breaks the execution. Solution must be correct!\n",
    "    exec(cell, globals(), env)\n",
    "    # Copy the modifications into submission object.\n",
    "    globals()['submission'] = SimpleNamespace(**env)\n",
    "    # Copy the modifications into globals().\n",
    "    for k in env:\n",
    "        globals()[k] = env[k]\n",
    "\n",
    "import sys\n",
    "import io\n",
    "        \n",
    "@register_line_magic\n",
    "def autotest(line):\n",
    "    \"\"\"Run the unit tests inline\n",
    "    \n",
    "    Returns the result object. result.results is a summary dictionary of outcomes.\n",
    "    result.errors \n",
    "    \"\"\"\n",
    "    \n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(eval(line))\n",
    "    errors = io.StringIO()\n",
    "    result = unittest.TextTestRunner(verbosity=4, stream=errors, resultclass=SummaryTestResult).run(suite) \n",
    "    return result, errors.getvalue()\n",
    "\n",
    "# Remove the names from the global namespace.\n",
    "del autotest, submission, solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hidecode"
    ]
   },
   "outputs": [],
   "source": [
    "# MASTER ONLY. TODO(salikh): Extract the magics code into a pip library.\n",
    "\n",
    "from jinja2 import Template\n",
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import HtmlFormatter\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "@register_cell_magic\n",
    "def template(line, cell):\n",
    "    \"\"\"Registers a template for report generation.\n",
    "    \n",
    "    Use {{results['TestClassName.test_method']}} to extract specific outcomes and be prepared\n",
    "    for the keys to be absent in the results map, if the test could not be run at all (e.g. because\n",
    "    of syntax error in the submission).\n",
    "    \"\"\"\n",
    "    name = line\n",
    "    if line == '':\n",
    "        name = 'report_template'\n",
    "    # Define a Jinja2 template based on cell contents.\n",
    "    globals()[name] = Template(cell)\n",
    "\n",
    "@register_cell_magic\n",
    "def report(line, cell):\n",
    "    \"\"\"Renders the named template.\n",
    "    \n",
    "    Syntax:\n",
    "      %%report results_var\n",
    "      template_name\n",
    "    \"\"\"\n",
    "    var_name = line\n",
    "    template_name = cell\n",
    "    template = eval(template_name, globals())\n",
    "    results = eval(var_name, globals())\n",
    "    highlighted_source = highlight(submission_source.source, PythonLexer(), HtmlFormatter())\n",
    "    # Render the template giving the specified variable as 'results',\n",
    "    # and render the result as inlined HTML in cell output. 'source' is\n",
    "    # the prerendered source code.\n",
    "    return HTML(template.render(results=results, source=highlighted_source))\n",
    "\n",
    "# Remove the names from the global namespace.\n",
    "del template, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: printing greeting\n",
    "\n",
    "```\n",
    "# The markdown cell with triple-backtick block matching /^# EXERCISE METADATA/ is an exercise-level\n",
    "# metadata. The next block is assumed to be the solution block, and will get annotated with\n",
    "# the exercise_id.\n",
    "# EXERCISE METADATA\n",
    "exercise_id: \"hello1\"\n",
    "```\n",
    "\n",
    "Now it is your turn. Please create a program in the next cell that would print a message \"Hello, world\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world\n"
     ]
    }
   ],
   "source": [
    "%%solution\n",
    "\"\"\" # BEGIN PROMPT\n",
    "# ... put your program here\n",
    "\"\"\" # END PROMPT\n",
    "# BEGIN SOLUTION\n",
    "print(\"Hello, world\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_has_hello (__main__.HelloOutputTest) ... ok\n",
      "test_not_empty (__main__.HelloOutputTest) ... ok\n",
      "test_output (__main__.HelloOutputTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.000s\n",
      "\n",
      "OK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will not be included in the student notebook because of BEGIN UNITTEST marker below.\n",
    "\n",
    "# The test below assumes that the solution above was written into two files:\n",
    "# - submission.py with the solution code\n",
    "# - submission_source.py which defines a single variable source whose value holds the source\n",
    "#   code of the submission.\n",
    "#\n",
    "# and then imported with\n",
    "#\n",
    "#   import submission_source\n",
    "#   import submission\n",
    "#\n",
    "# In the Jupyter notebook, this setup is performed by %%solution and %%submission magics.\n",
    "\n",
    "# BEGIN UNITTEST\n",
    "# The unit tests main part is contained between \"BEGIN UNITTEST\" and \"END UNITTEST\". It will be copied\n",
    "# verbatim into the autograder directory, with an addition of 'import submission' and 'import submission_source'\n",
    "import unittest\n",
    "#import submission_source\n",
    "\n",
    "import sys\n",
    "import io\n",
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "\n",
    "# TODO(salikh): Move the helper code into a library.\n",
    "@contextmanager\n",
    "def capture_output():\n",
    "    \"\"\"Captures the stdout and stderr into StringIO objects.\"\"\"\n",
    "    capture_out, capture_err = StringIO(), StringIO()\n",
    "    save_out, save_err = sys.stdout, sys.stderr\n",
    "    try:\n",
    "        sys.stdout, sys.stderr = capture_out, capture_err\n",
    "        yield sys.stdout, sys.stderr\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = save_out, save_err\n",
    "\n",
    "class HelloOutputTest(unittest.TestCase):\n",
    "    def test_output(self):\n",
    "        with capture_output() as (out, err):\n",
    "            exec(submission_source.source)\n",
    "        self.assertEqual(err.getvalue(), \"\")\n",
    "        self.assertEqual(out.getvalue(), \"Hello, world\\n\")\n",
    "        \n",
    "    def test_not_empty(self):\n",
    "        with capture_output() as (out, err):\n",
    "            exec(submission_source.source)\n",
    "        self.assertNotEqual(out.getvalue(), \"\")\n",
    "        \n",
    "    def test_has_hello(self):\n",
    "        with capture_output() as (out, err):\n",
    "            exec(submission_source.source)\n",
    "        self.assertTrue(\"Hello\" in out.getvalue())\n",
    "\n",
    "# END UNITTEST\n",
    "\n",
    "# The parts after END UNITTEST are executed in the notebook environment, but not copied\n",
    "# to the autograder scripts or to student notebooks.\n",
    "\n",
    "result, log = %autotest HelloOutputTest\n",
    "# Optional. Useful for debugging.\n",
    "print(log)\n",
    "assert(result.results['HelloOutputTest.test_output'] == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template HelloOutputTest_template\n",
    "<h2 style='color: #387;'>Your submission</h2>\n",
    "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
    "{{ source }}\n",
    "</pre>\n",
    "<h2 style='color: #387;'>Results</h2>\n",
    "{% if not results['HelloOutputTest.test_not_empty'] %}\n",
    "Your snippet does not produce any output. Please add a print statement.\n",
    "{% elif not results['HelloOutputTest.test_has_hello'] %}\n",
    "The output of your code does not include \"Hello\" string. Please add it.\n",
    "{% elif not results['HelloOutputTest.test_output'] %}\n",
    "The output is incorrect. Please make sure it is exactly \"Hello, world\".\n",
    "{% else %}\n",
    "Your code looks okay.\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2 style='color: #387;'>Your submission</h2>\n",
       "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Hello, world&quot;</span><span class=\"p\">)</span>\n",
       "</pre></div>\n",
       "\n",
       "</pre>\n",
       "<h2 style='color: #387;'>Results</h2>\n",
       "\n",
       "Your code looks okay.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%report result.results\n",
    "HelloOutputTest_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: returning greeting as value\n",
    "\n",
    "```\n",
    "# EXERCISE METADATA\n",
    "exercise_id: \"hello2\"\n",
    "```\n",
    "\n",
    "Please create a function that given a name string returns a string with a greeting,\n",
    "For example, for input `\"world\"` it should return `\"Hello, world\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%solution\n",
    "def hello(name):\n",
    "    \"\"\" # BEGIN PROMPT\n",
    "    # Please put your solution here:\n",
    "    # return ...\n",
    "    pass\n",
    "    \"\"\" # END PROMPT\n",
    "    # BEGIN SOLUTION\n",
    "    return \"Hello, \" + name\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert(hello(\"world\") == \"Hello, world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A code cell marked with `\"# TEST\"` will be converted into a unit test for the solution. It will also be preserved in the student version of the notebook.\n",
    "\n",
    "##### MASTER ONLY\n",
    "\n",
    "`# MASTER ONLY` marker works in text cells too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_hello (__main__.HelloTest) ... ok\n",
      "test_includes_arg (__main__.HelloTest) ... ok\n",
      "test_includes_hello (__main__.HelloTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.000s\n",
      "\n",
      "OK\n",
      "\n",
      "{'HelloTest.test_includes_arg': True, 'HelloTest.test_hello': True, 'HelloTest.test_includes_hello': True}\n"
     ]
    }
   ],
   "source": [
    "# The part before \"BEGIN UNITTEST\" -- preamble -- sets up the environment so that 'submission.hello'\n",
    "# is a function that we need to test. In the autograder worker environment, the preamble will be\n",
    "# replaced with 'import submission' with an assumption that the student's solution will be written\n",
    "# to the file 'submission.py'. The submission source will be available in submission_source.py\n",
    "# that will define a single variable named 'source'.\n",
    "\n",
    "# The unit tests main part is contained between \"BEGIN UNITTEST\" and \"END UNITTEST\". It will be copied\n",
    "# verbatim into the autograder directory, with an addition of 'import submission' and 'import submission_source'\n",
    "\n",
    "# BEGIN UNITTEST\n",
    "#import submission\n",
    "\n",
    "import unittest\n",
    "import ast\n",
    "\n",
    "class HelloTest(unittest.TestCase):\n",
    "    \n",
    "    def test_hello(self):\n",
    "        self.assertEqual(submission.hello(\"one\"), \"Hello, one\")\n",
    "        \n",
    "    def test_includes_arg(self):\n",
    "        self.assertTrue(\"xyz123\" in submission.hello(\"xyz123\"))\n",
    "        \n",
    "    def test_includes_hello(self):\n",
    "        self.assertTrue(\"Hello\" in submission.hello(\"xyz123\"))\n",
    "\n",
    "# END UNITTEST\n",
    "\n",
    "result, log = %autotest HelloTest\n",
    "# Optional.\n",
    "print(log)\n",
    "print(result.results)\n",
    "assert result.results['HelloTest.test_hello'] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template HelloTest_template\n",
    "<h2 style='color: #387;'>Your submission</h2>\n",
    "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
    "{{ source }}\n",
    "</pre>\n",
    "<h2 style='color: #387;'>Results</h2>\n",
    "{% if not results['HelloTest.py'] %}\n",
    "Your code does not compile\n",
    "{% elif not results['HelloTest.test_includes_hello'] %}\n",
    "The response from your function does not include \"Hello\" string. Please check if you have included it.\n",
    "{% elif not results['HelloTest.test_includes_arg'] %}\n",
    "The response from your function does not include the person name, which as given as the function argument.\n",
    "{% elif not results['HelloTest.test_hello'] %}\n",
    "Something is wrong.\n",
    "{% else %}\n",
    "Your code looks okay.\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2 style='color: #387;'>Your submission</h2>\n",
       "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">hello</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">return</span> <span class=\"s2\">&quot;Hello, &quot;</span> <span class=\"o\">+</span> <span class=\"n\">name</span>\n",
       "</pre></div>\n",
       "\n",
       "</pre>\n",
       "<h2 style='color: #387;'>Results</h2>\n",
       "\n",
       "Your code looks okay.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%report result.results\n",
    "HelloTest_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_compiles (__main__.HelloSyntaxTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n",
      "\n",
      "{'HelloSyntaxTest.test_compiles': True}\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNITTEST\n",
    "#import submission_source\n",
    "import unittest\n",
    "import ast\n",
    "\n",
    "class HelloSyntaxTest(unittest.TestCase):\n",
    "    def test_compiles(self):\n",
    "        # Expect success or exception.\n",
    "        ast.parse(submission_source.source)\n",
    "        \n",
    "# END UNITTEST        \n",
    "\n",
    "result, log = %autotest HelloSyntaxTest\n",
    "# Optional.\n",
    "print(log)\n",
    "print(result.results)\n",
    "assert result.results['HelloSyntaxTest.test_compiles'] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template HelloSyntaxTest_template\n",
    "<style type='text/css'>\n",
    ".k { color: purple; }\n",
    ".c1 { color: green; }\n",
    ".s2 { color: brown; }\n",
    "</style>\n",
    "<h2 style='color: #387;'>Your submission</h2>\n",
    "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
    "{{ source }}\n",
    "</pre>\n",
    "<h2 style='color: #087;'>Results</h2>\n",
    "{% if not results['HelloSyntaxTest.test_compiles']: %}\n",
    "Do you even compile, bro?\n",
    "{% else %}\n",
    "Syntax looks fine.\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".k { color: purple; }\n",
       ".c1 { color: green; }\n",
       ".s2 { color: brown; }\n",
       "</style>\n",
       "<h2 style='color: #387;'>Your submission</h2>\n",
       "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">hello</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">return</span> <span class=\"s2\">&quot;Hello, &quot;</span> <span class=\"o\">+</span> <span class=\"n\">name</span>\n",
       "</pre></div>\n",
       "\n",
       "</pre>\n",
       "<h2 style='color: #087;'>Results</h2>\n",
       "\n",
       "Syntax looks fine.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%report result.results\n",
    "HelloSyntaxTest_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%submission\n",
    "# The tests below test that the unit test suite above produces expected outcomes from\n",
    "def hello(name):\n",
    "    return \"Bye, \" + name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1, log = %autotest HelloTest\n",
    "#print(log)\n",
    "assert result1.results['HelloTest.test_hello'] == False\n",
    "assert result1.results['HelloTest.test_includes_hello'] == False\n",
    "assert result1.results['HelloTest.test_includes_arg'] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2 style='color: #387;'>Your submission</h2>\n",
       "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"c1\"># The tests below test that the unit test suite above produces expected outcomes from</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">hello</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">return</span> <span class=\"s2\">&quot;Bye, &quot;</span> <span class=\"o\">+</span> <span class=\"n\">name</span>\n",
       "</pre></div>\n",
       "\n",
       "</pre>\n",
       "<h2 style='color: #387;'>Results</h2>\n",
       "\n",
       "The response from your function does not include \"Hello\" string. Please check if you have included it.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%report result1.results\n",
    "HelloTest_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2, log = %autotest HelloSyntaxTest\n",
    "assert result2.results['HelloSyntaxTest.test_compiles'] == True\n",
    "results = {**result1.results, **result2.results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".k { color: purple; }\n",
       ".c1 { color: green; }\n",
       ".s2 { color: brown; }\n",
       "</style>\n",
       "<h2 style='color: #387;'>Your submission</h2>\n",
       "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"c1\"># The tests below test that the unit test suite above produces expected outcomes from</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">hello</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">return</span> <span class=\"s2\">&quot;Bye, &quot;</span> <span class=\"o\">+</span> <span class=\"n\">name</span>\n",
       "</pre></div>\n",
       "\n",
       "</pre>\n",
       "<h2 style='color: #087;'>Results</h2>\n",
       "\n",
       "Syntax looks fine.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%report result2.results\n",
    "HelloSyntaxTest_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2 style='color: #387;'>Your submission</h2>\n",
       "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"c1\"># The tests below test that the unit test suite above produces expected outcomes from</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">hello</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">return</span> <span class=\"s2\">&quot;Bye, &quot;</span> <span class=\"o\">+</span> <span class=\"n\">name</span>\n",
       "</pre></div>\n",
       "\n",
       "</pre>\n",
       "<h2 style='color: #387;'>Results</h2>\n",
       "\n",
       "The response from your function does not include \"Hello\" string. Please check if you have included it.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%report result2.results\n",
    "HelloTest_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%submission\n",
    "def wrong_hello(name):\n",
    "    return \"Bye, \" + name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_compiles (__main__.HelloSyntaxTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n",
      "\n",
      "{'HelloSyntaxTest.test_compiles': True}\n"
     ]
    }
   ],
   "source": [
    "result, log = %autotest HelloSyntaxTest\n",
    "print(log)\n",
    "print(result.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while executing submission:\n",
      " invalid syntax (<string>, line 4)\n"
     ]
    }
   ],
   "source": [
    "%%submission\n",
    "# This submission has a syntax error. Note that invalid code inside %%submission cell\n",
    "# does not break the notebook execution.\n",
    "def syntax_error(name:\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASTER ONLY\n",
    "# In case of syntax error in submission, it is initiliazed to None value.\n",
    "assert submission == None\n",
    "# TODO(salikh): Fix the behavior, since in the autograder 'import submission' will fail,\n",
    "# producing empty outcome vector rather than a bunch of ERROR outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_compiles (__main__.HelloSyntaxTest) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_compiles (__main__.HelloSyntaxTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-17-991943cb22a2>\", line 9, in test_compiles\n",
      "    ast.parse(submission_source.source)\n",
      "  File \"/usr/lib/python3.5/ast.py\", line 35, in parse\n",
      "    return compile(source, filename, mode, PyCF_ONLY_AST)\n",
      "  File \"<unknown>\", line 4\n",
      "    return name\n",
      "         ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n",
      "\n",
      "{'HelloSyntaxTest.test_compiles': False}\n"
     ]
    }
   ],
   "source": [
    "# %autotest expects the submission to have been set with %%submission cell magic previously.\n",
    "result3, log = %autotest HelloSyntaxTest\n",
    "print(log)\n",
    "print(result3.results)\n",
    "assert(result3.results['HelloSyntaxTest.test_compiles'] == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".k { color: purple; }\n",
       ".c1 { color: green; }\n",
       ".s2 { color: brown; }\n",
       "</style>\n",
       "<h2 style='color: #387;'>Your submission</h2>\n",
       "<pre style='background: #F0F0F0; padding: 3pt; margin: 4pt; border: 1pt solid #DDD; border-radius: 3pt;'\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"c1\"># This submission has a syntax error. Note that invalid code inside %%submission cell</span>\n",
       "<span class=\"c1\"># does not break the notebook execution.</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">syntax_error</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">:</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">name</span>\n",
       "</pre></div>\n",
       "\n",
       "</pre>\n",
       "<h2 style='color: #087;'>Results</h2>\n",
       "\n",
       "Do you even compile, bro?\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%report result3.results\n",
    "HelloSyntaxTest_template"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
